{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9261292,"sourceType":"datasetVersion","datasetId":5603866}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport itertools\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import VGG16, VGG19, MobileNetV3Large, ResNet152V2, MobileNet\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\nNUM_CLASSES = 7\nBATCH_SIZE = 32\nEPOCHS = 10  # Increase the number of epochs for better training\n\ndata_generator = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2\n)\n\n# Function to create MobileNet model\ndef create_mobilenet_v3_large():\n    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\n    # Fine-tune only the last convolutional block\n    for layer in base_model.layers[:-4]:\n        layer.trainable = False\n\n    x = layers.Flatten()(base_model.output)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)  # Add dropout layer\n    predictions = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n\n    model = models.Model(inputs=base_model.input, outputs=predictions)\n    return model\n\n# Function to create VGG16 model\ndef create_vgg16():\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\n    # Fine-tune only the last convolutional block\n    for layer in base_model.layers[:-4]:\n        layer.trainable = False\n\n    x = layers.Flatten()(base_model.output)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)  # Add dropout layer\n    predictions = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n\n    model = models.Model(inputs=base_model.input, outputs=predictions)\n    return model\n\n# Function to create VGG19 model\ndef create_vgg19():\n    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\n    # Fine-tune only the last convolutional block\n    for layer in base_model.layers[:-4]:\n        layer.trainable = False\n\n    x = layers.Flatten()(base_model.output)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)  # Add dropout layer\n    predictions = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n\n    model = models.Model(inputs=base_model.input, outputs=predictions)\n    return model\n\n# Function to create ResNet152V2 model\ndef create_resnet152v2():\n    base_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\n    # Fine-tune only the last convolutional block\n    for layer in base_model.layers[:-4]:\n        layer.trainable = False\n\n    x = layers.Flatten()(base_model.output)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)  # Add dropout layer\n    predictions = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n\n    model = models.Model(inputs=base_model.input, outputs=predictions)\n    return model\n\ndef lr_scheduler(epoch, lr):\n    if epoch < 100:\n        return 0.001  # Keep the initial learning rate for the first 10 epochs\n    else:\n        return lr  # Keep the learning rate constant after the 10th epoch\n\ndef train_without_early_stopping(model, train_gen, val_gen, epochs, model_name):\n    history = model.fit(\n        train_gen,\n        steps_per_epoch=train_gen.samples // BATCH_SIZE,\n        epochs=epochs,\n        validation_data=val_gen,\n        validation_steps=val_gen.samples // BATCH_SIZE\n    )\n    return history\n\n\n# Function to extract F1-score, precision, and recall from the classification report\ndef extract_metrics(classification_rep):\n    class_names = list(validation_generator.class_indices.keys())\n    f1_scores = [classification_rep[emotion]['f1-score'] for emotion in class_names]\n    precisions = [classification_rep[emotion]['precision'] for emotion in class_names]\n    recalls = [classification_rep[emotion]['recall'] for emotion in class_names]\n    return f1_scores, precisions, recalls\n\n# Function to calculate metrics (F1-score, precision, and recall)\ndef calculate_metrics(model, test_generator):\n    y_true = test_generator.classes\n    y_pred = model.predict(test_generator)\n    y_pred = np.argmax(y_pred, axis=1)\n    # Calculate confusion matrix\n    confusion_mat = confusion_matrix(y_true, y_pred)\n    # Calculate classification report\n    class_names = list(validation_generator.class_indices.keys())\n    classification_rep = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n    return confusion_mat, classification_rep\n\n# Function to plot multiple confusion matrices\ndef plot_multiple_confusion_matrices_values(confusion_mats, model_names, emotions):\n    plt.figure(figsize=(10, 8))\n\n    for i, confusion_mat in enumerate(confusion_mats):\n        total_samples = np.sum(confusion_mat)\n        confusion_mat_percentage = confusion_mat / total_samples * 100  # Convert to percentage\n\n        plt.subplot(2, 2, i + 1)\n        plt.imshow(confusion_mat_percentage, interpolation='nearest', cmap=plt.cm.Blues)\n        plt.title(f'Confusion Matrix - {model_names[i]}')\n        plt.colorbar()\n        tick_marks = np.arange(len(emotions))\n        plt.xticks(tick_marks, emotions, rotation=45)\n        plt.yticks(tick_marks, emotions)\n        plt.xlabel('Predicted Emotion')\n        plt.ylabel('True Emotion')\n\n        for j, k in itertools.product(range(confusion_mat.shape[0]), range(confusion_mat.shape[1])):\n            plt.text(k, j, f'{confusion_mat_percentage[j, k]:.2f}', horizontalalignment=\"center\", color=\"white\" if confusion_mat_percentage[j, k] > 50 else \"black\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n# Function to plot the confusion matrix for multiple models\ndef plot_multiple_confusion_matrices(confusion_mats, model_names, emotions):\n    plt.figure(figsize=(10, 8))\n    for i, confusion_mat in enumerate(confusion_mats):\n        plt.subplot(2, 2, i + 1)\n        plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)\n        plt.title(f'Confusion Matrix - {model_names[i]}')\n        plt.colorbar()\n        tick_marks = np.arange(len(emotions))\n        plt.xticks(tick_marks, emotions, rotation=45)\n        plt.yticks(tick_marks, emotions)\n        plt.xlabel('Predicted Emotion')\n        plt.ylabel('True Emotion')\n\n        thresh = confusion_mat.max() / 2.0\n        for j, k in itertools.product(range(confusion_mat.shape[0]), range(confusion_mat.shape[1])):\n            plt.text(k, j, format(confusion_mat[j, k], 'd'), horizontalalignment=\"center\", color=\"white\" if confusion_mat[j, k] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.show()\n\n# Function to plot training history (loss and accuracy) for all models\ndef plot_training_history(model_histories, model_names, model_colors):\n    num_models = len(model_histories)\n\n    plt.figure(figsize=(12, 10))\n\n    # Plot Validation Loss\n    plt.subplot(2, 2, 1)\n    for i, (history, model_name) in enumerate(zip(model_histories, model_names)):\n        plt.plot(history.history['val_loss'], label=model_name, color=model_colors[i])\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Loss')\n    plt.title('Validation Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Training Loss\n    plt.subplot(2, 2, 2)\n    for i, (history, model_name) in enumerate(zip(model_histories, model_names)):\n        plt.plot(history.history['loss'], label=model_name, color=model_colors[i])\n    plt.xlabel('Epoch')\n    plt.ylabel('Training Loss')\n    plt.title('Training Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Validation Accuracy\n    plt.subplot(2, 2, 3)\n    for i, (history, model_name) in enumerate(zip(model_histories, model_names)):\n        plt.plot(history.history['val_accuracy'], label=model_name, color=model_colors[i])\n    plt.xlabel('Epoch')\n    plt.ylabel('Validation Accuracy')\n    plt.title('Validation Accuracy')\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Training Accuracy\n    plt.subplot(2, 2, 4)\n    for i, (history, model_name) in enumerate(zip(model_histories, model_names)):\n        plt.plot(history.history['accuracy'], label=model_name, color=model_colors[i])\n    plt.xlabel('Epoch')\n    plt.ylabel('Training Accuracy')\n    plt.title('Training Accuracy')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n# Set the random seed for reproducibility\ntf.random.set_seed(42)\n\ntrain_data_dir = '/kaggle/input/student-facial-expression-recognition/SFER dataset/SFER dataset/train'  # Replace with the path to your \"train\" folder\ntest_data_dir = '/kaggle/input/student-facial-expression-recognition/SFER dataset/SFER dataset/test'    # Replace with the path to your \"test\" folder\n\ntrain_generator = data_generator.flow_from_directory(\n    train_data_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True\n)\n\nvalidation_generator = data_generator.flow_from_directory(\n    train_data_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False\n)\n\ntest_generator = data_generator.flow_from_directory(\n    test_data_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Define class names\nclass_names = list(train_generator.class_indices.keys())\n\n# Create models\nmobilenet_v3_large_model = create_mobilenet_v3_large()\nvgg16_model = create_vgg16()\nvgg19_model = create_vgg19()\nresnet152v2_model = create_resnet152v2()\n\n# Compile models\nmobilenet_v3_large_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nvgg16_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nvgg19_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nresnet152v2_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define callbacks to save best weights\nmobilenet_checkpoint = ModelCheckpoint('mobilenet_best1_weights.h5', save_best_only=True)\nvgg16_checkpoint = ModelCheckpoint('vgg16_best1_weights.h5', save_best_only=True)\nvgg19_checkpoint = ModelCheckpoint('vgg19_best1_weights.h5', save_best_only=True)\nresnet152v2_checkpoint = ModelCheckpoint('resnet152v2_best1_weights.h5', save_best_only=True)\n\n# Train models with the checkpoint callbacks\nmobilenet_history = mobilenet_v3_large_model.fit(\n    train_generator, \n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=validation_generator,  \n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    callbacks=[mobilenet_checkpoint]\n)\n\nvgg16_history = vgg16_model.fit(\n    train_generator, \n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=validation_generator,  \n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    callbacks=[vgg16_checkpoint]\n)\n\nvgg19_history = vgg19_model.fit(\n    train_generator, \n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=validation_generator, \n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    callbacks=[vgg19_checkpoint]\n)\n\nresnet152v2_history = resnet152v2_model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=validation_generator,  \n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    callbacks=[resnet152v2_checkpoint]\n)\n\n# Load best weights for evaluation\nmobilenet_v3_large_model.load_weights('mobilenet_best1_weights.h5')\nvgg16_model.load_weights('vgg16_best1_weights.h5')\nvgg19_model.load_weights('vgg19_best1_weights.h5')\nresnet152v2_model.load_weights('resnet152v2_best1_weights.h5')\n\n# Function to calculate mean metrics\ndef calculate_mean_metrics(history):\n    mean_val_loss = np.mean(history.history['val_loss'])\n    mean_train_loss = np.mean(history.history['loss'])\n    mean_val_acc = np.mean(history.history['val_accuracy'])\n    mean_train_acc = np.mean(history.history['accuracy'])\n    return mean_val_loss, mean_train_loss, mean_val_acc, mean_train_acc\n\n\n# Calculate metrics for all models and save to the list\nmobilenet_metrics = calculate_mean_metrics(mobilenet_history)\nvgg16_metrics = calculate_mean_metrics(vgg16_history)\nvgg19_metrics = calculate_mean_metrics(vgg19_history)\nresnet152v2_metrics = calculate_mean_metrics(resnet152v2_history)\n\n\n# Calculate metrics for all models\nmobilenet_confusion_mat, mobilenet_classification_rep = calculate_metrics(mobilenet_v3_large_model, test_generator)\nvgg16_confusion_mat, vgg16_classification_rep = calculate_metrics(vgg16_model, test_generator)\nvgg19_confusion_mat, vgg19_classification_rep = calculate_metrics(vgg19_model, test_generator)\nresnet152v2_confusion_mat, resnet152v2_classification_rep = calculate_metrics(resnet152v2_model, test_generator)\n\n# Extract F1-score, precision, and recall for all models\nmobilenet_f1, mobilenet_precision, mobilenet_recall = extract_metrics(mobilenet_classification_rep)\nvgg16_f1, vgg16_precision, vgg16_recall = extract_metrics(vgg16_classification_rep)\nvgg19_f1, vgg19_precision, vgg19_recall = extract_metrics(vgg19_classification_rep)\nresnet152v2_f1, resnet152v2_precision, resnet152v2_recall = extract_metrics(resnet152v2_classification_rep)\n\n# Define colors for bars\nmobilenet_color = 'limegreen'\nvgg16_color = 'dodgerblue'\nvgg19_color = 'darkorange'\nresnet152v2_color = 'orchid'  # Add a new color for ResNet152v2\n\n# Define model names for the bar plot\nmodel_names = ['MobileNet', 'VGG16', 'VGG19', 'ResNet152V2']\n\n# Plot F1-score, precision, recall, and accuracy for all models on a single graph\nplt.figure(figsize=(16, 12))\n\nbar_width = 0.15\nbar_positions = np.arange(len(class_names))\n\n# Plot F1-score\nplt.subplot(2, 2, 1)\nplt.bar(bar_positions - 1.5 * bar_width, mobilenet_f1, width=bar_width, label='MobileNet', align='center', color=mobilenet_color)\nplt.bar(bar_positions - 0.5 * bar_width, vgg16_f1, width=bar_width, label='VGG16', align='center', color=vgg16_color)\nplt.bar(bar_positions + 0.5 * bar_width, vgg19_f1, width=bar_width, label='VGG19', align='center', color=vgg19_color)\nplt.bar(bar_positions + 1.5 * bar_width, resnet152v2_f1, width=bar_width, label='ResNet152V2', align='center', color=resnet152v2_color)\nplt.xticks(bar_positions, class_names)\nplt.xlabel('Emotion')\nplt.ylabel('F1-score')\nplt.title('F1-score Comparison')\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Plot Precision\nplt.subplot(2, 2, 2)\nplt.bar(bar_positions - 1.5 * bar_width, mobilenet_precision, width=bar_width, label='MobileNet', align='center', color=mobilenet_color)\nplt.bar(bar_positions - 0.5 * bar_width, vgg16_precision, width=bar_width, label='VGG16', align='center', color=vgg16_color)\nplt.bar(bar_positions + 0.5 * bar_width, vgg19_precision, width=bar_width, label='VGG19', align='center', color=vgg19_color)\nplt.bar(bar_positions + 1.5 * bar_width, resnet152v2_precision, width=bar_width, label='ResNet152V2', align='center', color=resnet152v2_color)\nplt.xticks(bar_positions, class_names)\nplt.xlabel('Emotion')\nplt.ylabel('Precision')\nplt.title('Precision Comparison')\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Plot Recall\nplt.subplot(2, 2, 3)\nplt.bar(bar_positions - 1.5 * bar_width, mobilenet_recall, width=bar_width, label='MobileNet', align='center', color=mobilenet_color)\nplt.bar(bar_positions - 0.5 * bar_width, vgg16_recall, width=bar_width, label='VGG16', align='center', color=vgg16_color)\nplt.bar(bar_positions + 0.5 * bar_width, vgg19_recall, width=bar_width, label='VGG19', align='center', color=vgg19_color)\nplt.bar(bar_positions + 1.5 * bar_width, resnet152v2_recall, width=bar_width, label='ResNet152V2', align='center', color=resnet152v2_color)\nplt.xticks(bar_positions, class_names)\nplt.xlabel('Emotion')\nplt.ylabel('Recall')\nplt.title('Recall Comparison')\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Plot Accuracy\nplt.subplot(2, 2, 4)\nmodel_accuracies = [mobilenet_history.history['val_accuracy'][-1],\n                    vgg16_history.history['val_accuracy'][-1],\n                    vgg19_history.history['val_accuracy'][-1],\n                    resnet152v2_history.history['val_accuracy'][-1]]\n\n# Adjust the width of the bars to make them thicker\nbar_width = 0.2 # You can increase this value to make the bars thicker\nplt.bar(model_names, model_accuracies, width=bar_width, color=[mobilenet_color, vgg16_color, vgg19_color, resnet152v2_color])\n\nplt.xlabel('Model')\nplt.ylabel('Validation Accuracy')\nplt.title('Validation Accuracy Comparison')\nplt.grid(axis='y', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# Plot the confusion matrix for all models\nconfusion_mats = [mobilenet_confusion_mat, vgg16_confusion_mat, vgg19_confusion_mat, resnet152v2_confusion_mat]\nmodel_names = ['MobileNet', 'VGG16', 'VGG19', 'ResNet152V2']\nplot_multiple_confusion_matrices_values(confusion_mats, model_names, emotions=class_names)\n\n# Plot the training history for all models\nmodel_colors = ['limegreen', 'dodgerblue', 'darkorange', 'orchid']  # Update with the colors you prefer\nmodel_histories = [mobilenet_history, vgg16_history, vgg19_history, resnet152v2_history]\nplot_training_history(model_histories, model_names, model_colors)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-28T01:24:42.147310Z","iopub.execute_input":"2024-08-28T01:24:42.148416Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Found 5343 images belonging to 7 classes.\nFound 1334 images belonging to 7 classes.\nFound 1399 images belonging to 7 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]}]}